{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d87d231-606f-4309-9438-88b590db8ec3",
   "metadata": {},
   "source": [
    "## Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea2513-8b45-4951-a01f-8fa49726dbfd",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) is a supervised machine learning algorithm used for both classification and regression. Though we say regression problems as well itâ€™s best suited for classification. The main objective of the SVM algorithm is to find the optimal hyperplane in an N-dimensional space that can separate the data points in different classes in the feature space. The hyperplane tries that the margin between the closest points of different classes should be as maximum as possible. The dimension of the hyperplane depends upon the number of features. If the number of input features is two, then the hyperplane is just a line. If the number of input features is three, then the hyperplane becomes a 2-D plane. It becomes difficult to imagine when the number of features exceeds three. \n",
    "\n",
    "\\begin{aligned} \\text{Linear : } K(w,b) &= w^Tx+b \\\\ \\text{Polynomial : } K(w,x) &= (\\gamma w^Tx+b)^N \\\\ \\text{Gaussian RBF: } K(w,x) &= \\exp(-\\gamma|| x_i-x_j||^n \\\\ \\text{Sigmoid :} K(x_i, x_j) &= \\tanh(\\alpha x_i^Tx_j + b) \\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f1ea9-6a41-4a2d-9c71-b27655abf130",
   "metadata": {},
   "source": [
    "## Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5229e-a639-4eed-aeff-98b5a99def10",
   "metadata": {},
   "source": [
    "## inear SVM: Linear SVMs use a linear decision boundary to separate the data points of different classes. When the data can be precisely linearly separated, linear SVMs are very suitable. This means that a single straight line (in 2D) or a hyperplane (in higher dimensions) can entirely divide the data points into their respective classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b675677-e12b-4df2-9fb7-a0d71c0c9545",
   "metadata": {},
   "source": [
    "## Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde8504-1342-4c56-a2a7-47da49acdc48",
   "metadata": {},
   "source": [
    "### The kernel trick in SVM allows for effective handling of non-linear patterns by using kernel functions to implicitly map data into higher-dimensional spaces, where non-linear boundaries can be linearly separated without explicit computation of the transformation. This enhances SVM's capability to classify complex datasets without the computational overhead of transforming data explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568434e-0202-4508-b14a-4a5c679fa05e",
   "metadata": {},
   "source": [
    "## Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483070d-b6d4-498e-aa5b-160091c81c53",
   "metadata": {},
   "source": [
    "## SVM algorithm finds the closest point of the lines from both the classes. These points are called support vectors. The distance between the vectors and the hyperplane is called as margin. And the goal of SVM is to maximize this margin. The hyperplane with maximum margin is called the optimal hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736bfb3b-4262-4eb1-bd68-a13c44710e37",
   "metadata": {},
   "source": [
    "## Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aba501-35e5-4abf-ad68-170b887555b0",
   "metadata": {},
   "source": [
    "## Here's a brief overview with diagrams:\n",
    "\n",
    "1. **Hyperplane:** Linear boundary separating classes.\n",
    "   \n",
    "2. **Margin:** Region around the hyperplane.\n",
    "   - **Hard Margin:** No points within the margin.\n",
    "   - **Soft Margin:** Allows some points within, controlled by \\( C \\).\n",
    "\n",
    "3. **Support Vectors:** Points on or within the margin influencing the hyperplane.\n",
    "\n",
    "4. **Soft vs. Hard Margin:** \n",
    "   - **Hard Margin:** No points within.\n",
    "   - **Soft Margin:** Allows some misclassification within.\n",
    "   \n",
    "These concepts illustrate how SVM finds optimal separation between classes, adapting to data separability and regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3df771-18fe-4584-aa42-70367f80794a",
   "metadata": {},
   "source": [
    "## Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "### ~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "### ~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "### ~ Compute the accuracy of the model on the testing setl\n",
    "### ~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "### ~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879e574-b8f5-4fcd-8b58-adbd988127d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
